# 2周

## Amdahl法则

## Meaning of Amdahl’s Law 阿姆达尔定律
- Speedup is primarily affected by the execution time taken up by non-parallelizable parts of the program
-> 加速比主要受程序中不可并行化部分所占用的执行时间的影响.

- Even if the part that accounts for 95% of the sequential execution time is parallelized using 32,768 processors, the speedup is only about 20 times
-> 即使用32768个处理器并行化占顺序执行95的部分, 加速比也只有20倍左右

根据技术发展的趋势进行学习是很重要的.不要满足于已经知道的编程模型

## Process
Process: the instance of a computer program that is being executed
-> 进程: 正在执行的计算机程序的实例
– Name given by the operating system for management purposes
-> 操作系统为管理目的而给出的名称
– Have its own address space
-> 有自己的地址空间
– Composed of multiple threads
-> 由多个线程组成
– User process vs. kernel process
-> 用户进程 vs 内核进程

Usually, multiple processes exist on one computer system
-> 一个计算机系统上通常存在多个进程
Communication and synchronization with other processes is achieved by
sending and receiving messages
-> 与其他进程的通信和同步是通过发送和接受信息来实现的
– Message passing parallel programming
-> 消息传递并行编程

When there is only one core, the CPU moves around one process at a time to
process instructions from multiple processes
-> 当只有一个核心时, CPU一次移动一个进程来处理多个进程的指令
– Concurrent execution
-> 并发执行
– Instructions from one process are interleaved with instructions from another
processor
-> 来自一个进程的指令与来自另一个处理器的指令交叉执行
– Context switch
-> 上下文切换

## Context switch 上下文切换

Essential for multitasking
-> 多任务处理的必要条件
The process by which the operating system saves and restores the state of a thread
or process
- When scheduled on a CPU by the operating system, execution continues from the point where execution was previously interrupted
-> 当被操作系统安排在CPU上时, 执行从先前中断执行的地方继续执行
- Overhead due to context switches
上下文切换带来额外的开销

State of a thread or process
-> 线程或进程的状态
- Program counter and other registers
-> 程序计数器和其他寄存器
- Specialized data needed by the operating system
-> 操作系统需要的专用数据

## Thread 线程

可由调度程序独立管理的最小程序指令序列

## Schedule of Processes and Threads in Multi-Core 多核中的进程和线程调度

The operating system treats each core as a different processor
-> 操作系统将每个核心视为不同的处理器

The operating system's scheduler schedules multiple threads or processes on
different cores
-> 操作系统的调度程序在不同的内核上调度多个线程或进程
- On each core, threads or processes run in a time-slicing manner
-> 在每个核心上, 线程或进程以时间切片的方式运行

Because the core on which threads are scheduled varies depending on when the
program is executed, if multiple threads are executed, the results may vary each time
they are executed
-> 因为调度线程的核心取决于程序执行的时间, 如果执行多个线程, 每次执行的结果都可能不同

## Communication and Synchronization between Threads 线程之间的通信和同步

Other threads can detect some changes made to shared memory by one thread
-> 其他线程可以检测到一个线程对共享内存所做的一些更改

Synchronization and communication are possible by reading and writing memory in
the same location
-> 通过读取和写入同一位置的内存, 可以实现同步和通信

Debugging is difficult because the schedule of threads by the operating system is
non-deterministic
-> 调试困难, 因为操作系统的线程调度是不确定的

## Data Dependences 数据依赖性

Define the execution order relationship between different operations
- Any execution order that maintains data dependency outputs the same results as the original

program
- But it limits parallelism

Statement Q has data dependency on statement P
- There are possible execution paths where P and Q access the same memory location.
- The execution of P occurs before the execution of Q.

### Flow(true) dependence 流依赖

True dependence 真正的依赖
When P writes to a memory location and then Q reads that location (read after
write)
-> 当P写入内存位置, 然后Q读取该位置(写后读)

### Anti dependence 反依赖

False dependence 虚假依赖
当P读取内存位置, 然后Q写入该位置时(先读后写)

### Output dependence 输出依赖
False dependence 虚假依赖
When P writes to a memory location and then Q writs back to that location (write
after write)
-> 当P写入内存位置, 然后Q写回该位置时(写后写)


### Input dependence 输入依赖
not really a dependence 不是真正的依赖
When P reads a memory location and then Q reads that location again (read after
read)
-> 当P读取一个内存位置,然后Q再次读取该位置(一次又一次的读取)
## Parallelism 并行性

ILP: Instruction Level Parallelism 指令级并行性
TLP: Task Level Parallelism 任务级并行性
DLP: Data Level Parallelism数据级并行性

### ILP: Instruction Level Parallelism 指令级并行性

Applications’ features 应用程序的特性
Average number of instruction that can be executed simultaneously on a superscalar processor for each clock cycle 超标量处理器在每个时钟周期内可以同时执行的平均指令数
Dependences 依赖性

The ILP in the application is limited.
应用程序中的ILP是有限的.
- Even if the processor can execute N instructions simultaneously, if the ILP in the
application i less than N, it is a waste of hardware.
-> 即使处理器可以同时执行N条指令, 如果用用程序中的ILP I(指令)小于N, 则是浪费硬件.

### TLP: Task Level Parallelism 任务级并行性
Execute different types of tasks simultaneously 同时执行不同类型的任务
Divide the application into multiple parallel tasks 将应用程序划分为多个并行任务
Most applications have a very small number of parallel tasks 大多数应用程序都有非常少量的并行任务


### DLP: Data Level Parallelism数据级并行性
Also known as loop-level parallelism 也称循环级并行
Apply the same operation to different data items simultaneously 同时对不同的数据应用相同的操作
More data increases parallelism更多的数据增加并行性


### SIMD
Single Instruction, Multiple Data 单指令多数据

Computers with multiple processing elements that perform the same operation on
multiple data points simultaneously
-> 具有多个处理单元的计算机, 可以同时对多个数据点执行相同的操作
- One program counter 一个程序计数器

Compilers usually vectorize automatically 编译器通常自动向量化
## Synchronization 同步
Main Reasons for Synchronization 同步的主要原因
- To determine the order in which threads or processes are executed
-> 确定线程和进程执行的顺序
- To achieve mutual exclusion 实现互斥
	- Ensuring that multiple processes or threads cannot execute certain sections of code, often referred to as critical sections, simultaneously -> 确保多个进程或线程不能同时执行默写代码片段, 通常称为临界区


### Data Race 数据竞争
Two or more threads or processes access the same memory location concurrently,
and at least one of the accesses is a write operation
-> 两个或多个线程或进程并发访问相同的内存位置, 并且至少有一个访问是写操作

It occurs under the following conditions: 在以下条件发生
- Two or more threads access the same memory location 两个或多个线程访问相同的内存位置
- One of the threads writes 其中一个线程写
- When accessing that memory location is not protected using a synchronization method 访问该内存位置时没有使用同步方法进行保护

Access order is non-deterministic 访问顺序是不确定的
• May output different results each time it is executed

Some data races are used intentionally  有些数据竞争是有意的
- Busy-wait synchronization (spinlock synchronization) 繁忙等待同步(自旋锁同步)
- A thread repeatedly checks a condition to determine if it proceed with its execution一个线程反复检查一个条件, 以确定它是否继续执行
- Lock-free algorithm 无锁定算法
- Allowing multiple threads to operate on shared data without traditional locking mechanisms such as mutexes or semaphores, to prevent conflict 允许多个线程在没有传统锁机制(如互斥量或信号量)的情况下操作共享内数据, 以防止冲突

But in most cases, data races are bugs 但在大多数情况下, 数据竞争是BUG

### Atomic Counter using Lock 原子性和互斥性
!!!!!!!!!!!!!!!!!!!!!!!!!!!

### Thread Safety锁定和解锁
!!!!!!!!!!!!!!!!!!!!!!!!!!!

### Thread Safety 线程安全
!!!!!!!!!!!!!!!!!!!!!!!!!!!

## Barrier 屏障
– A thread that reaches a point where a barrier is present stops execution and waits for
all other threads to reach the same barrier before continuing execution.
• Commonly used when executing the same function (code) at the same time
– SPMD

## 缓存一致性
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# 3周

# 5周

### 编译指令

通过pragma强制编译器优化方法

`#pragma unroll <n>`
- 展开紧接n次迭代的循环.
- 如果n不存在, 则展开整个循环

循环展开技巧:
进行(大量)循环展开并不总是能提高性能. 必须通过实验进行优化, 考虑权衡.

优点:
- 减少分支开销(条件语句检查, 分支)
- 编译器启发式地确定展开大小.
- 手动应用时, 可以通过实验采用一个良好的尺寸

缺点:
- 指令数量增加, 代码本身的大小也增加
- 特别是, 指令缓存缺失发生率增加

### 分支执行
当线程束中的多个线程的执行路径不同时出现分支执行

分支执行存在不必要地闲置一些线程的问题. 需要避免分支执行, 所有线程必须确定条件语句为 true或false

### 内存合并
将多个内存访问(读/写)合并为一个请求

GPU在warp执行的操作:
- 内存访问也是.
- 32个内存指令需要处理

最新的内存架构可以用单个请求处理连续数据
- 32个内存 4字节内存读请求
- 一个128-byte的内存读请求

条件:
- warp中的线程必须访问连续的内存
- 内存访问必须对其到128-byte的地址

提示: 内存合并对性能有重大影响, 应该是尝试的第一个内核优化目标

###