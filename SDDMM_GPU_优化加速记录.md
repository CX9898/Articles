# SDDMM_GPU 优化加速记录

---

## 加入 Tensor core

使用 Tensor core 直接对密集矩阵乘法进行计算, 计算时间

### 过程中发现的问题或优化点

#### 1:

函数 `__shfl_xor` 在 cuda 后续版本已经弃用, 应该改为使用 `__shfl_xor_sync`

但是发现使用 `__shfl_xor` 和 `__shfl_xor_sync` 的结果不一致

```c++
sm1 += __shfl_xor(sm1, 1); // 使用shuffle指令. 使线程0的sm1加到线程1的sm1上, 线程1的sm1加到线程2的sm2上
sm2 += __shfl_xor(sm2, 1);
```

```c++
sm1 += __shfl_xor_sync(0xFFFFFFFF, sm1, 1); // 使用shuffle指令. 使线程0的sm1加到线程1的sm1上, 线程1的sm1加到线程2的sm2上
sm2 += __shfl_xor_sync(0xFFFFFFFF, sm2, 1);
```

#### 2:

核函数中加入了 `if()` 判断语句, 就算永远执行单一分支也比不加入 `if()` 判断语句时耗时久

加入判断语句(用时: 31.388 ms):

```c++
const int ldp = N;
const auto pOffsetPtr = matrixP + pRowId * ldp + pColId;
const float sparsity = calculateMatrixTileSparsity(WMMA_M, WMMA_N, ldp, MatrixStorageOrder::row_major, pOffsetPtr);
if (sparsity < 0) { // Always false

} else {
matrixTileMultiplicationUseTensorCore(pRowId, pColId, M, N, K, matrixA, matrixB, matrixS, matrixP);
}
```

不加入判断语句(用时: 21.8441 ms):

```c++
const int ldp = N;
const auto pOffsetPtr = matrixP + pRowId * ldp + pColId;
const float sparsity = calculateMatrixTileSparsity(WMMA_M, WMMA_N, ldp, MatrixStorageOrder::row_major, pOffsetPtr);
//    if (sparsity < 0) { // Always false

//    } else {
matrixTileMultiplicationUseTensorCore(pRowId, pColId, M, N, K, matrixA, matrixB, matrixS, matrixP);
//    }
```

#### 3:

blockDim 的尺寸改变的话时间大幅度降低.

原来的尺寸是 `block.x = 32` `block.y = 32`

```
M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%
openTensorCoreMode
openTensorCoreMode matrixA : row = 37008, col = 256
openTensorCoreMode matrixB : row = 256, col = 37008
openTensorCoreMode matrixS : row = 37008, col = 37008
grid : [2313 73 1] block : [32 32 1]
Func comp_sddmm_gpu time : 23.8314 ms
```

改成了 `block.x = 128` `block.y = 4`

```
M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%
openTensorCoreMode
openTensorCoreMode matrixA : row = 37008, col = 256
openTensorCoreMode matrixB : row = 256, col = 37008
openTensorCoreMode matrixS : row = 37008, col = 37008
grid : [579 579 1] block : [128 4 1]
Func comp_sddmm_gpu time : 13.0769 ms
```

从 grid 的数量上看, 改成128和4时, grid 数量大大增加了, 原先 2313 × 73 = 168849, 变成 579 × 579 = 335241

#### 4: `print()`

在CPU中使用 `printf()` 函数会隐式转换, 而在GPU中使用 `printf()` 函数不会隐式转换, 需要显示转换. 例如:

```c++
    size_t numWarpX, numWarpY, numWarps;
printf(" numWarpX = %d, numWarpY = %d, numWarps = %d\n",
static_cast<int>(numWarpX), static_cast<int>(numWarpY), static_cast<int>(numWarps));
```

---

### 最初版本 测试结果

#### 测试结果 Debug build, 所有矩阵都是行主序储存

- GPU : 4090
- Debug build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 16 × 16 × 16

| GPU : 4090, Debug build                                              | sddmm_isratnisa |  sddmm_zcx  |
|:---------------------------------------------------------------------|:---------------:|:-----------:|
| M : 1504, N : 1504, K : 256,<br/>nnz : 746316, sparsity : 67.0066%   |   2.13123 ms    | 0.059168 ms |
| M : 12432, N : 12432, K : 256,<br/>nnz : 746316, sparsity : 99.5171% |   0.419488 ms   | 3.06074 ms  |
| M : 8000, N : 8000, K : 256,<br/>nnz : 640000, sparsity : 99%        |   0.488832 ms   |  1.2759 ms  |
| M : 8000, N : 8000, K = 256,<br/>nnz: 1280000, sparsity : 98%        |   0.817376 ms   |    위에 같다    |
| M : 8000, N : 8000, K : 256,<br/>nnz : 1632000, sparsity : 97.45%    |   0.998016 ms   |    위에 같다    |
| M : 8000, N : 8000, K : 256,<br/>nnz : 1920000, sparsity : 97%       |   1.14563 ms    |    위에 같다    |
| M : 8000, N : 8000, K : 256,<br/>nnz : 2240000, sparsity : 96.5%     |   1.30128 ms    |    위에 같다    |
| M : 8000, N : 8000, K : 256,<br/> nnz : 2560000, sparsity : 96%      |   1.47882 ms    |    위에 같다    |
| M : 8000, N : 8000, K : 256,<br/>nnz : 6400000, sparsity : 90%       |   5.10989 ms    |    위에 같다    |

#### Release build

- Debug build
- GPU : 4090
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major

| 4090 |     |     |
|------|-----|-----|
|      |     |     |
|      |     |     |
|      |     |     |

---

### 使计算支持各个尺寸的矩阵(Matrix::openTensorCoreMode())

#### 测试结果 行主序储存 16×16×16

- GPU : 4090
- Debug build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 16 × 16 × 16

| GPU:4090, debug build, row_major,16×16×16                            | sddmm_isratnisa | sddmm_zcx   |
|----------------------------------------------------------------------|-----------------|-------------|
| M : 3000, N : 7000, K : 256, nnz : 313110, sparsity : 98.51%         | 0.508448 ms     | 0.415744 ms |
| M : 2000, N : 12000, K : 256, nnz : 746000, sparsity : 96.8917%      | 1.59091 ms      | 0.448512 ms |
| M : 300000, N : 103000, K : 256, nnz : 69000000, sparsity : 99.7767% | 26.6559 ms      |             |
| M : 35000, N : 35000, K : 256, nnz : 422000, sparsity : 99.9656%     | 0.237888 ms     | 21.9749 ms  |
| M : 549000, N : 549000, K : 256, nnz : 926000, sparsity : 99.9997%   | 2.61411 ms      |             |
| M : 426000, N : 426000, K : 256, nnz : 1000000, sparsity : 99.9995%  | 2.41718 ms      |             |
| M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%     | 0.234112 ms     | 23.9791 ms  |
| M : 4000, N : 4000, K : 256, nnz : 88000, sparsity : 99.45%          | 0.172064 ms     | 0.317472 ms |
| M : 106000, N : 106000, K : 256, nnz : 3000000, sparsity : 99.9733%  | 1.97805 ms      |             |
| M : 685000, N : 685000, K : 256, nnz : 8000000, sparsity : 99.9983%  | 12.3796 ms      |             |
| M : 916000, N : 916000, K : 256, nnz : 5000000, sparsity : 99.9994%  | 9.60342 ms      |             |
| M : 326000, N : 326000, K : 256, nnz : 1000000, sparsity : 99.9991%  | 2.11952 ms      |             |
| M : 197000, N : 197000, K : 256, nnz : 2000000, sparsity : 99.9948%  | 2.10243 ms      |             |
| M : 390000, N : 390000, K : 256, nnz : 2000000, sparsity : 99.9987%  | 3.45181 ms      |             |
| M : 260000, N : 260000, K : 256, nnz : 4000000, sparsity : 99.9941%  | 4.01792 ms      |             |
| M : 241000, N : 241000, K : 256, nnz : 561000, sparsity : 99.999%    | 1.28944 ms      |             |
| M : 36000, N : 36000, K : 256, nnz : 4000000, sparsity : 99.6914%    | 1.33123 ms      | 23.1115 ms  |

由于将稀疏矩阵按照0也储存的方式储存, 导致矩阵太大的情况下内存分配错误, 使得计算失败

#### 测试结果 行主序储存 32×8×16

- GPU : 4090
- Debug build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 32×8×16

| GPU:4090, debug build, row_major, 32×8×16                            | sddmm_isratnisa | sddmm_zcx   |
|----------------------------------------------------------------------|-----------------|-------------|
| M : 3000, N : 7000, K : 256, nnz : 313110, sparsity : 98.51%         | 0.508448 ms     | 0.372704 ms |
| M : 2000, N : 12000, K : 256, nnz : 746000, sparsity : 96.8917%      | 1.59091 ms      | 0.44336 ms  |
| M : 300000, N : 103000, K : 256, nnz : 69000000, sparsity : 99.7767% | 26.6559 ms      |             |
| M : 35000, N : 35000, K : 256, nnz : 422000, sparsity : 99.9656%     | 0.237888 ms     | 17.8151 ms  |
| M : 549000, N : 549000, K : 256, nnz : 926000, sparsity : 99.9997%   | 2.61411 ms      |             |
| M : 426000, N : 426000, K : 256, nnz : 1000000, sparsity : 99.9995%  | 2.41718 ms      |             |
| M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%     | 0.234112 ms     | 19.9316 ms  |
| M : 4000, N : 4000, K : 256, nnz : 88000, sparsity : 99.45%          | 0.172064 ms     | 0.317472 ms |
| M : 106000, N : 106000, K : 256, nnz : 3000000, sparsity : 99.9733%  | 1.97805 ms      |             |
| M : 685000, N : 685000, K : 256, nnz : 8000000, sparsity : 99.9983%  | 12.3796 ms      |             |
| M : 916000, N : 916000, K : 256, nnz : 5000000, sparsity : 99.9994%  | 9.60342 ms      |             |
| M : 326000, N : 326000, K : 256, nnz : 1000000, sparsity : 99.9991%  | 2.11952 ms      |             |
| M : 197000, N : 197000, K : 256, nnz : 2000000, sparsity : 99.9948%  | 2.10243 ms      |             |
| M : 390000, N : 390000, K : 256, nnz : 2000000, sparsity : 99.9987%  | 3.45181 ms      |             |
| M : 260000, N : 260000, K : 256, nnz : 4000000, sparsity : 99.9941%  | 4.01792 ms      |             |
| M : 241000, N : 241000, K : 256, nnz : 561000, sparsity : 99.999%    | 1.28944 ms      |             |
| M : 36000, N : 36000, K : 256, nnz : 4000000, sparsity : 99.6914%    | 1.33123 ms      | 19.9169 ms  |

由于将稀疏矩阵按照0也储存的方式储存, 导致矩阵太大的情况下内存分配错误, 使得计算失败

使用 32×8×16 的维度 比 16×16×16 更快了

#### 测试结果 行主序储存 8×32×16

- GPU : 4090
- Debug build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 8×32×16

| GPU:4090, debug build, row_major, 8×32×16                            | sddmm_isratnisa | sddmm_zcx   |
|----------------------------------------------------------------------|-----------------|-------------|
| M : 3000, N : 7000, K : 256, nnz : 313110, sparsity : 98.51%         | 0.508448 ms     | 0.567232 ms |
| M : 2000, N : 12000, K : 256, nnz : 746000, sparsity : 96.8917%      | 1.59091 ms      | 0.577088 ms |
| M : 300000, N : 103000, K : 256, nnz : 69000000, sparsity : 99.7767% | 26.6559 ms      |             |
| M : 35000, N : 35000, K : 256, nnz : 422000, sparsity : 99.9656%     | 0.237888 ms     | 30.5323 ms  |
| M : 549000, N : 549000, K : 256, nnz : 926000, sparsity : 99.9997%   | 2.61411 ms      |             |
| M : 426000, N : 426000, K : 256, nnz : 1000000, sparsity : 99.9995%  | 2.41718 ms      |             |
| M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%     | 0.234112 ms     | 34.361 ms   |
| M : 4000, N : 4000, K : 256, nnz : 88000, sparsity : 99.45%          | 0.172064 ms     | 0.398336 ms |
| M : 106000, N : 106000, K : 256, nnz : 3000000, sparsity : 99.9733%  | 1.97805 ms      |             |
| M : 685000, N : 685000, K : 256, nnz : 8000000, sparsity : 99.9983%  | 12.3796 ms      |             |
| M : 916000, N : 916000, K : 256, nnz : 5000000, sparsity : 99.9994%  | 9.60342 ms      |             |
| M : 326000, N : 326000, K : 256, nnz : 1000000, sparsity : 99.9991%  | 2.11952 ms      |             |
| M : 197000, N : 197000, K : 256, nnz : 2000000, sparsity : 99.9948%  | 2.10243 ms      |             |
| M : 390000, N : 390000, K : 256, nnz : 2000000, sparsity : 99.9987%  | 3.45181 ms      |             |
| M : 260000, N : 260000, K : 256, nnz : 4000000, sparsity : 99.9941%  | 4.01792 ms      |             |
| M : 241000, N : 241000, K : 256, nnz : 561000, sparsity : 99.999%    | 1.28944 ms      |             |
| M : 36000, N : 36000, K : 256, nnz : 4000000, sparsity : 99.6914%    | 1.33123 ms      | 32.917 ms   |

由于将稀疏矩阵按照0也储存的方式储存, 导致矩阵太大的情况下内存分配错误, 使得计算失败

使用 8×32×16 的维度 比 16×16×16 和 32×8×16 更慢了

---

### 在上个版本的基础上将 blockDIm 的尺寸改变成 128×4

blockDim 的尺寸改变的话时间大幅度降低.

#### 测试结果 行主序储存 16×16×16

- GPU : 4090
- Debug build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 16 × 16 × 16

| GPU:4090, debug build, row_major,16×16×16                            | sddmm_isratnisa | sddmm_zcx   |
|----------------------------------------------------------------------|-----------------|-------------|
| M : 3000, N : 7000, K : 256, nnz : 313110, sparsity : 98.51%         | 0.508448 ms     | 0.280928 ms |
| M : 2000, N : 12000, K : 256, nnz : 746000, sparsity : 96.8917%      | 1.59091 ms      | 0.294144 ms |
| M : 300000, N : 103000, K : 256, nnz : 69000000, sparsity : 99.7767% | 26.6559 ms      |             |
| M : 35000, N : 35000, K : 256, nnz : 422000, sparsity : 99.9656%     | 0.237888 ms     | 16.2021 ms  |
| M : 549000, N : 549000, K : 256, nnz : 926000, sparsity : 99.9997%   | 2.61411 ms      |             |
| M : 426000, N : 426000, K : 256, nnz : 1000000, sparsity : 99.9995%  | 2.41718 ms      |             |
| M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%     | 0.234112 ms     | 13.0718 ms  |
| M : 4000, N : 4000, K : 256, nnz : 88000, sparsity : 99.45%          | 0.172064 ms     | 0.189664 ms |
| M : 106000, N : 106000, K : 256, nnz : 3000000, sparsity : 99.9733%  | 1.97805 ms      |             |
| M : 685000, N : 685000, K : 256, nnz : 8000000, sparsity : 99.9983%  | 12.3796 ms      |             |
| M : 916000, N : 916000, K : 256, nnz : 5000000, sparsity : 99.9994%  | 9.60342 ms      |             |
| M : 326000, N : 326000, K : 256, nnz : 1000000, sparsity : 99.9991%  | 2.11952 ms      |             |
| M : 197000, N : 197000, K : 256, nnz : 2000000, sparsity : 99.9948%  | 2.10243 ms      |             |
| M : 390000, N : 390000, K : 256, nnz : 2000000, sparsity : 99.9987%  | 3.45181 ms      |             |
| M : 260000, N : 260000, K : 256, nnz : 4000000, sparsity : 99.9941%  | 4.01792 ms      |             |
| M : 241000, N : 241000, K : 256, nnz : 561000, sparsity : 99.999%    | 1.28944 ms      |             |
| M : 36000, N : 36000, K : 256, nnz : 4000000, sparsity : 99.6914%    | 1.33123 ms      | 14.1067 ms  |

由于将稀疏矩阵按照0也储存的方式储存, 导致矩阵太大的情况下内存分配错误, 使得计算失败

#### 测试结果 行主序储存 32×8×16

- GPU : 4090
- Debug build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 32×8×16

| GPU:4090, debug build, row_major, 32×8×16                            | sddmm_isratnisa | sddmm_zcx   |
|----------------------------------------------------------------------|-----------------|-------------|
| M : 3000, N : 7000, K : 256, nnz : 313110, sparsity : 98.51%         | 0.508448 ms     | 0.33904 ms  |
| M : 2000, N : 12000, K : 256, nnz : 746000, sparsity : 96.8917%      | 1.59091 ms      | 0.417888 ms |
| M : 300000, N : 103000, K : 256, nnz : 69000000, sparsity : 99.7767% | 26.6559 ms      |             |
| M : 35000, N : 35000, K : 256, nnz : 422000, sparsity : 99.9656%     | 0.237888 ms     | 16.5034 ms  |
| M : 549000, N : 549000, K : 256, nnz : 926000, sparsity : 99.9997%   | 2.61411 ms      |             |
| M : 426000, N : 426000, K : 256, nnz : 1000000, sparsity : 99.9995%  | 2.41718 ms      |             |
| M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%     | 0.234112 ms     | 18.4755 ms  |
| M : 4000, N : 4000, K : 256, nnz : 88000, sparsity : 99.45%          | 0.172064 ms     | 0.2536 ms   |
| M : 106000, N : 106000, K : 256, nnz : 3000000, sparsity : 99.9733%  | 1.97805 ms      |             |
| M : 685000, N : 685000, K : 256, nnz : 8000000, sparsity : 99.9983%  | 12.3796 ms      |             |
| M : 916000, N : 916000, K : 256, nnz : 5000000, sparsity : 99.9994%  | 9.60342 ms      |             |
| M : 326000, N : 326000, K : 256, nnz : 1000000, sparsity : 99.9991%  | 2.11952 ms      |             |
| M : 197000, N : 197000, K : 256, nnz : 2000000, sparsity : 99.9948%  | 2.10243 ms      |             |
| M : 390000, N : 390000, K : 256, nnz : 2000000, sparsity : 99.9987%  | 3.45181 ms      |             |
| M : 260000, N : 260000, K : 256, nnz : 4000000, sparsity : 99.9941%  | 4.01792 ms      |             |
| M : 241000, N : 241000, K : 256, nnz : 561000, sparsity : 99.999%    | 1.28944 ms      |             |
| M : 36000, N : 36000, K : 256, nnz : 4000000, sparsity : 99.6914%    | 1.33123 ms      | 18.8892 ms  |

由于将稀疏矩阵按照0也储存的方式储存, 导致矩阵太大的情况下内存分配错误, 使得计算失败

blockDim使用128×8的情况下, 使用 32×8×16 的维度 比 16×16×16 更慢了

#### 测试结果 行主序储存 8×32×16

- GPU : 4090
- Debug build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 8×32×16

| GPU:4090, debug build, row_major, 8×32×16                            | sddmm_isratnisa | sddmm_zcx   |
|----------------------------------------------------------------------|-----------------|-------------|
| M : 3000, N : 7000, K : 256, nnz : 313110, sparsity : 98.51%         | 0.508448 ms     | 0.311552 ms |
| M : 2000, N : 12000, K : 256, nnz : 746000, sparsity : 96.8917%      | 1.59091 ms      | 0.326208 ms |
| M : 300000, N : 103000, K : 256, nnz : 69000000, sparsity : 99.7767% | 26.6559 ms      |             |
| M : 35000, N : 35000, K : 256, nnz : 422000, sparsity : 99.9656%     | 0.237888 ms     | 20.6243 ms  |
| M : 549000, N : 549000, K : 256, nnz : 926000, sparsity : 99.9997%   | 2.61411 ms      |             |
| M : 426000, N : 426000, K : 256, nnz : 1000000, sparsity : 99.9995%  | 2.41718 ms      |             |
| M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%     | 0.234112 ms     | 16.8907 ms  |
| M : 4000, N : 4000, K : 256, nnz : 88000, sparsity : 99.45%          | 0.172064 ms     | 0.214592 ms |
| M : 106000, N : 106000, K : 256, nnz : 3000000, sparsity : 99.9733%  | 1.97805 ms      |             |
| M : 685000, N : 685000, K : 256, nnz : 8000000, sparsity : 99.9983%  | 12.3796 ms      |             |
| M : 916000, N : 916000, K : 256, nnz : 5000000, sparsity : 99.9994%  | 9.60342 ms      |             |
| M : 326000, N : 326000, K : 256, nnz : 1000000, sparsity : 99.9991%  | 2.11952 ms      |             |
| M : 197000, N : 197000, K : 256, nnz : 2000000, sparsity : 99.9948%  | 2.10243 ms      |             |
| M : 390000, N : 390000, K : 256, nnz : 2000000, sparsity : 99.9987%  | 3.45181 ms      |             |
| M : 260000, N : 260000, K : 256, nnz : 4000000, sparsity : 99.9941%  | 4.01792 ms      |             |
| M : 241000, N : 241000, K : 256, nnz : 561000, sparsity : 99.999%    | 1.28944 ms      |             |
| M : 36000, N : 36000, K : 256, nnz : 4000000, sparsity : 99.6914%    | 1.33123 ms      | 16.0298 ms  |

由于将稀疏矩阵按照0也储存的方式储存, 导致矩阵太大的情况下内存分配错误, 使得计算失败

blockDim使用128×8的情况下, 使用 8×32×16 的维度有的变快有的变慢

---

### TensorCore 测试 warp 中数据储存格式

#### Matrix A (16×16) (FP16):

|    | 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15  |
|----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| 0  | 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15  |
| 1  | 16  | 17  | 18  | 19  | 20  | 21  | 22  | 23  | 24  | 25  | 26  | 27  | 28  | 29  | 30  | 31  |
| 2  | 32  | 33  | 34  | 35  | 36  | 37  | 38  | 39  | 40  | 41  | 42  | 43  | 44  | 45  | 46  | 47  |
| 3  | 48  | 49  | 50  | 51  | 52  | 53  | 54  | 55  | 56  | 57  | 58  | 59  | 60  | 61  | 62  | 63  |
| 4  | 64  | 65  | 66  | 67  | 68  | 69  | 70  | 71  | 72  | 73  | 74  | 75  | 76  | 77  | 78  | 79  |
| 5  | 80  | 81  | 82  | 83  | 84  | 85  | 86  | 87  | 88  | 89  | 90  | 91  | 92  | 93  | 94  | 95  |
| 6  | 96  | 97  | 98  | 99  | 100 | 101 | 102 | 103 | 104 | 105 | 106 | 107 | 108 | 109 | 110 | 111 |
| 7  | 112 | 113 | 114 | 115 | 116 | 117 | 118 | 119 | 120 | 121 | 122 | 123 | 124 | 125 | 126 | 127 |
| 8  | 128 | 129 | 130 | 131 | 132 | 133 | 134 | 135 | 136 | 137 | 138 | 139 | 140 | 141 | 142 | 143 |
| 9  | 144 | 145 | 146 | 147 | 148 | 149 | 150 | 151 | 152 | 153 | 154 | 155 | 156 | 157 | 158 | 159 |
| 10 | 160 | 161 | 162 | 163 | 164 | 165 | 166 | 167 | 168 | 169 | 170 | 171 | 172 | 173 | 174 | 175 |
| 11 | 176 | 177 | 178 | 179 | 180 | 181 | 182 | 183 | 184 | 185 | 186 | 187 | 188 | 189 | 190 | 191 |
| 12 | 192 | 193 | 194 | 195 | 196 | 197 | 198 | 199 | 200 | 201 | 202 | 203 | 204 | 205 | 206 | 207 |
| 13 | 208 | 209 | 210 | 211 | 212 | 213 | 214 | 215 | 216 | 217 | 218 | 219 | 220 | 221 | 222 | 223 |
| 14 | 224 | 225 | 226 | 227 | 228 | 229 | 230 | 231 | 232 | 233 | 234 | 235 | 236 | 237 | 238 | 239 |
| 15 | 240 | 241 | 242 | 243 | 244 | 245 | 246 | 247 | 248 | 249 | 250 | 251 | 252 | 253 | 254 | 255 |

#### Matrix B (16×16) (FP16):

|    | 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15  |
|----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| 0  | 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15  |
| 1  | 16  | 17  | 18  | 19  | 20  | 21  | 22  | 23  | 24  | 25  | 26  | 27  | 28  | 29  | 30  | 31  |
| 2  | 32  | 33  | 34  | 35  | 36  | 37  | 38  | 39  | 40  | 41  | 42  | 43  | 44  | 45  | 46  | 47  |
| 3  | 48  | 49  | 50  | 51  | 52  | 53  | 54  | 55  | 56  | 57  | 58  | 59  | 60  | 61  | 62  | 63  |
| 4  | 64  | 65  | 66  | 67  | 68  | 69  | 70  | 71  | 72  | 73  | 74  | 75  | 76  | 77  | 78  | 79  |
| 5  | 80  | 81  | 82  | 83  | 84  | 85  | 86  | 87  | 88  | 89  | 90  | 91  | 92  | 93  | 94  | 95  |
| 6  | 96  | 97  | 98  | 99  | 100 | 101 | 102 | 103 | 104 | 105 | 106 | 107 | 108 | 109 | 110 | 111 |
| 7  | 112 | 113 | 114 | 115 | 116 | 117 | 118 | 119 | 120 | 121 | 122 | 123 | 124 | 125 | 126 | 127 |
| 8  | 128 | 129 | 130 | 131 | 132 | 133 | 134 | 135 | 136 | 137 | 138 | 139 | 140 | 141 | 142 | 143 |
| 9  | 144 | 145 | 146 | 147 | 148 | 149 | 150 | 151 | 152 | 153 | 154 | 155 | 156 | 157 | 158 | 159 |
| 10 | 160 | 161 | 162 | 163 | 164 | 165 | 166 | 167 | 168 | 169 | 170 | 171 | 172 | 173 | 174 | 175 |
| 11 | 176 | 177 | 178 | 179 | 180 | 181 | 182 | 183 | 184 | 185 | 186 | 187 | 188 | 189 | 190 | 191 |
| 12 | 192 | 193 | 194 | 195 | 196 | 197 | 198 | 199 | 200 | 201 | 202 | 203 | 204 | 205 | 206 | 207 |
| 13 | 208 | 209 | 210 | 211 | 212 | 213 | 214 | 215 | 216 | 217 | 218 | 219 | 220 | 221 | 222 | 223 |
| 14 | 224 | 225 | 226 | 227 | 228 | 229 | 230 | 231 | 232 | 233 | 234 | 235 | 236 | 237 | 238 | 239 |
| 15 | 240 | 241 | 242 | 243 | 244 | 245 | 246 | 247 | 248 | 249 | 250 | 251 | 252 | 253 | 254 | 255 |

#### Matrix C (16×16) (FP32):

|    | 0      | 1      | 2      | 3      | 4      | 5      | 6      | 7      | 8      | 9      | 10     | 11     | 12     | 13     | 14     | 15     |
|----|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| 0  | 19840  | 19960  | 20080  | 20200  | 20320  | 20440  | 20560  | 20680  | 20800  | 20920  | 21040  | 21160  | 21280  | 21400  | 21520  | 21640  |
| 1  | 50560  | 50936  | 51312  | 51688  | 52064  | 52440  | 52816  | 53192  | 53568  | 53944  | 54320  | 54696  | 55072  | 55448  | 55824  | 56200  |
| 2  | 81280  | 81912  | 82544  | 83176  | 83808  | 84440  | 85072  | 85704  | 86336  | 86968  | 87600  | 88232  | 88864  | 89496  | 90128  | 90760  |
| 3  | 112000 | 112888 | 113776 | 114664 | 115552 | 116440 | 117328 | 118216 | 119104 | 119992 | 120880 | 121768 | 122656 | 123544 | 124432 | 125320 |
| 4  | 142720 | 143864 | 145008 | 146152 | 147296 | 148440 | 149584 | 150728 | 151872 | 153016 | 154160 | 155304 | 156448 | 157592 | 158736 | 159880 |
| 5  | 173440 | 174840 | 176240 | 177640 | 179040 | 180440 | 181840 | 183240 | 184640 | 186040 | 187440 | 188840 | 190240 | 191640 | 193040 | 194440 |
| 6  | 204160 | 205816 | 207472 | 209128 | 210784 | 212440 | 214096 | 215752 | 217408 | 219064 | 220720 | 222376 | 224032 | 225688 | 227344 | 229000 |
| 7  | 234880 | 236792 | 238704 | 240616 | 242528 | 244440 | 246352 | 248264 | 250176 | 252088 | 254000 | 255912 | 257824 | 259736 | 261648 | 263560 |
| 8  | 265600 | 267768 | 269936 | 272104 | 274272 | 276440 | 278608 | 280776 | 282944 | 285112 | 287280 | 289448 | 291616 | 293784 | 295952 | 298120 |
| 9  | 296320 | 298744 | 301168 | 303592 | 306016 | 308440 | 310864 | 313288 | 315712 | 318136 | 320560 | 322984 | 325408 | 327832 | 330256 | 332680 |
| 10 | 327040 | 329720 | 332400 | 335080 | 337760 | 340440 | 343120 | 345800 | 348480 | 351160 | 353840 | 356520 | 359200 | 361880 | 364560 | 367240 |
| 11 | 357760 | 360696 | 363632 | 366568 | 369504 | 372440 | 375376 | 378312 | 381248 | 384184 | 387120 | 390056 | 392992 | 395928 | 398864 | 401800 |
| 12 | 388480 | 391672 | 394864 | 398056 | 401248 | 404440 | 407632 | 410824 | 414016 | 417208 | 420400 | 423592 | 426784 | 429976 | 433168 | 436360 |
| 13 | 419200 | 422648 | 426096 | 429544 | 432992 | 436440 | 439888 | 443336 | 446784 | 450232 | 453680 | 457128 | 460576 | 464024 | 467472 | 470920 |
| 14 | 449920 | 453624 | 457328 | 461032 | 464736 | 468440 | 472144 | 475848 | 479552 | 483256 | 486960 | 490664 | 494368 | 498072 | 501776 | 505480 |
| 15 | 480640 | 484600 | 488560 | 492520 | 496480 | 500440 | 504400 | 508360 | 512320 | 516280 | 520240 | 524200 | 528160 | 532120 | 536080 | 540040 |

#### 结论

实际测试出的 warp 中每一个线程所储存的数据代表的行列:

##### M = 16, N = 16, K = 16

warp 中每个线程计算后的数据

| landId\idx |   0    |   1    |   2    |   3    |   4    |   5    |   6    |   7    |
|:----------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
|     T0     | 19840  | 19960  | 265600 | 267768 | 20800  | 20920  | 282944 | 285112 |
|     T1     | 20080  | 20200  | 269936 | 272104 | 21040  | 21160  | 287280 | 289448 |
|     T2     | 20320  | 20440  | 274272 | 276440 | 21280  | 21400  | 291616 | 293784 |
|     T3     | 20560  | 20680  | 278608 | 280776 | 21520  | 21640  | 295952 | 298120 |
|     T4     | 50560  | 50936  | 296320 | 298744 | 53568  | 53944  | 315712 | 318136 |
|     T5     | 51312  | 51688  | 301168 | 303592 | 54320  | 54696  | 320560 | 322984 |
|     T6     | 52064  | 52440  | 306016 | 308440 | 55072  | 55448  | 325408 | 327832 |
|     T7     | 52816  | 53192  | 310864 | 313288 | 55824  | 56200  | 330256 | 332680 |
|     T8     | 81280  | 81912  | 327040 | 329720 | 86336  | 86968  | 348480 | 351160 |
|     T9     | 82544  | 83176  | 332400 | 335080 | 87600  | 88232  | 353840 | 356520 |
|    T10     | 83808  | 84440  | 337760 | 340440 | 88864  | 89496  | 359200 | 361880 |
|    T11     | 85072  | 85704  | 343120 | 345800 | 90128  | 90760  | 364560 | 367240 |
|    T12     | 112000 | 112888 | 357760 | 360696 | 119104 | 119992 | 381248 | 384184 |
|    T13     | 113776 | 114664 | 363632 | 366568 | 120880 | 121768 | 387120 | 390056 |
|    T14     | 115552 | 116440 | 369504 | 372440 | 122656 | 123544 | 392992 | 395928 |
|    T15     | 117328 | 118216 | 375376 | 378312 | 124432 | 125320 | 398864 | 401800 |
|    T16     | 142720 | 143864 | 388480 | 391672 | 151872 | 153016 | 414016 | 417208 |
|    T17     | 145008 | 146152 | 394864 | 398056 | 154160 | 155304 | 420400 | 423592 |
|    T18     | 147296 | 148440 | 401248 | 404440 | 156448 | 157592 | 426784 | 429976 |
|    T19     | 149584 | 150728 | 407632 | 410824 | 158736 | 159880 | 433168 | 436360 |
|    T20     | 173440 | 174840 | 419200 | 422648 | 184640 | 186040 | 446784 | 450232 |
|    T21     | 176240 | 177640 | 426096 | 429544 | 187440 | 188840 | 453680 | 457128 |
|    T22     | 179040 | 180440 | 432992 | 436440 | 190240 | 191640 | 460576 | 464024 |
|    T23     | 181840 | 183240 | 439888 | 443336 | 193040 | 194440 | 467472 | 470920 |
|    T24     | 204160 | 205816 | 449920 | 453624 | 217408 | 219064 | 479552 | 483256 |
|    T25     | 207472 | 209128 | 457328 | 461032 | 220720 | 222376 | 486960 | 490664 |
|    T26     | 210784 | 212440 | 464736 | 468440 | 224032 | 225688 | 494368 | 498072 |
|    T27     | 214096 | 215752 | 472144 | 475848 | 227344 | 229000 | 501776 | 505480 |
|    T28     | 234880 | 236792 | 480640 | 484600 | 250176 | 252088 | 512320 | 516280 |
|    T29     | 238704 | 240616 | 488560 | 492520 | 254000 | 255912 | 520240 | 524200 |
|    T30     | 242528 | 244440 | 496480 | 500440 | 257824 | 259736 | 528160 | 532120 |
|    T31     | 246352 | 248264 | 504400 | 508360 | 261648 | 263560 | 536080 | 540040 |

对应的列表:

| landId\idx |   0   |   1   |   2    |   3    |   4    |   5    |    6    |    7    |
|:----------:|:-----:|:-----:|:------:|:------:|:------:|:------:|:-------:|:-------:|
|     T0     | [0,0] | [0,1] | [8,0]  | [8,1]  | [0,8]  | [0,9]  |  [8,8]  |  [8,9]  |
|     T1     | [0,2] | [0,3] | [8,2]  | [8,3]  | [0,10] | [0,11] | [8,10]  | [8,11]  |
|     T2     | [0,4] | [0,5] | [8,4]  | [8,5]  | [0,12] | [0,13] | [8,12]  | [8,13]  |
|     T3     | [0,6] | [0,7] | [8,6]  | [8,7]  | [0,14] | [0,15] | [8,14]  | [8,15]  |
|     T4     | [1,0] | [1,1] | [9,0]  | [9,1]  | [1,8]  | [1,9]  |  [9,8]  |  [9,9]  |
|     T5     | [1,2] | [1,3] | [9,2]  | [9,3]  | [1,10] | [1,11] | [9,10]  | [9,11]  |
|     T6     | [1,4] | [1,5] | [9,4]  | [9,5]  | [1,12] | [1,13] | [9,12]  | [9,13]  | 
|     T7     | [1,6] | [1,7] | [9,6]  | [9,7]  | [1,14] | [1,15] | [9,14]  | [9,15]  |
|     T8     | [2,0] | [2,1] | [10,0] | [10,1] | [2,8]  | [2,9]  | [10,8]  | [10,9]  |
|     T9     | [2,2] | [2,3] | [10,2] | [10,3] | [2,10] | [2,11] | [10,10] | [10,11] |
|    T10     | [2,4] | [2,5] | [10,4] | [10,5] | [2,14] | [2,15] | [10,12] | [10,13] |    
|    T11     | [2,6] | [2,7] | [10,6] | [10,7] | [2,16] | [2,17] | [10,14] | [10,15] |
|    T12     | [3,0] | [3,1] | [11,0] | [11,1] | [3,8]  | [3,9]  | [11,8]  | [11,9]  |
|    T13     | [3,2] | [3,3] | [11,2] | [11,3] | [3,10] | [3,11] | [11,10] | [11,11] |
|    T14     | [3,4] | [3,5] | [11,4] | [11,5] | [3,12] | [3,13] | [11,12] | [11,13] |
|    T15     | [3,6] | [3,7] | [11,6] | [11,7] | [3,14] | [3,15] | [11,14] | [11,15] |
|    T16     | [4,0] | [4,1] | [12,0] | [12,1] | [4,8]  | [4,9]  | [12,8]  | [12,9]  |
|    T17     | [4,2] | [4,3] | [12,2] | [12,3] | [4,10] | [4,11] | [12,10] | [12,11] |
|    T18     | [4,4] | [4,5] | [12,4] | [12,5] | [4,12] | [4,13] | [12,12] | [12,13] |
|    T19     | [4,6] | [4,7] | [12,6] | [12,7] | [4,14] | [4,15] | [12,14] | [12,15] |
|    T20     | [5,0] | [5,1] | [13,0] | [13,1] | [5,8]  | [5,9]  | [13,8]  | [13,9]  |
|    T21     | [5,2] | [5,3] | [13,2] | [13,3] | [5,10] | [5,11] | [13,10] | [13,11] |
|    T22     | [5,4] | [5,5] | [13,4] | [13,5] | [5,12] | [5,13] | [13,12] | [13,13] |
|    T23     | [5,6] | [5,7] | [13,6] | [13,7] | [5,14] | [5,15] | [13,14] | [13,15] |
|    T24     | [6,0] | [6,1] | [14,0] | [14,1] | [6,8]  | [6,9]  | [14,8]  | [14,9]  |
|    T25     | [6,2] | [6,3] | [14,2] | [14,3] | [6,10] | [6,11] | [14,10] | [14,11] |
|    T26     | [6,4] | [6,5] | [14,4] | [14,5] | [6,12] | [6,13] | [14,12] | [14,13] |
|    T27     | [6,6] | [6,7] | [14,6] | [14,7] | [6,14] | [6,15] | [14,14] | [14,15] |
|    T28     | [7,0] | [7,1] | [15,0] | [15,1] | [7,8]  | [7,9]  | [15,8]  | [15,9]  |
|    T29     | [7,2] | [7,3] | [15,2] | [15,3] | [7,10] | [7,11] | [15,10] | [15,11] |
|    T30     | [7,4] | [7,5] | [15,4] | [15,5] | [7,12] | [7,13] | [15,12] | [15,13] |
|    T31     | [7,6] | [7,7] | [15,6] | [15,7] | [7,14] | [7,15] | [15,14] | [15,15] |

##### M = 32, N = 8, K = 32

| landId\idx |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |
|:----------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|     T0     | [0,0] | [0,1] | [8,0] | [8,1] | [0,8] | [0,9] | [8,8] | [8,9] |

##### M = 8, N = 32, K = 32

| landId\idx |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |
|:----------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|     T0     | [0,0] | [0,1] | [8,0] | [8,1] | [0,8] | [0,9] | [8,8] | [8,9] |

---

### 使用COO格式储存稀疏矩阵进行运算(第一版)

#### 测试结果 行主序储存 16×16×16

- GPU : 4090
- Debug build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 16 × 16 × 16

| GPU:4090, debug build, row_major,16×16×16                            | sddmm_isratnisa | sddmm_zcx  |
|----------------------------------------------------------------------|-----------------|------------|
| M : 3000, N : 7000, K : 256, nnz : 313110, sparsity : 98.51%         | 0.508448 ms     | 642.824 ms |
| M : 2000, N : 12000, K : 256, nnz : 746000, sparsity : 96.8917%      | 1.59091 ms      | 1735.09 ms |
| M : 300000, N : 103000, K : 256, nnz : 69000000, sparsity : 99.7767% | 26.6559 ms      |            |
| M : 35000, N : 35000, K : 256, nnz : 422000, sparsity : 99.9656%     | 0.237888 ms     |            |
| M : 549000, N : 549000, K : 256, nnz : 926000, sparsity : 99.9997%   | 2.61411 ms      |            |
| M : 426000, N : 426000, K : 256, nnz : 1000000, sparsity : 99.9995%  | 2.41718 ms      |            |
| M : 37000, N : 37000, K : 256, nnz : 368000, sparsity : 99.9731%     | 0.234112 ms     | 47499.1 ms |
| M : 4000, N : 4000, K : 256, nnz : 88000, sparsity : 99.45%          | 0.172064 ms     |            |
| M : 106000, N : 106000, K : 256, nnz : 3000000, sparsity : 99.9733%  | 1.97805 ms      |            |
| M : 685000, N : 685000, K : 256, nnz : 8000000, sparsity : 99.9983%  | 12.3796 ms      |            |
| M : 916000, N : 916000, K : 256, nnz : 5000000, sparsity : 99.9994%  | 9.60342 ms      |            |
| M : 326000, N : 326000, K : 256, nnz : 1000000, sparsity : 99.9991%  | 2.11952 ms      |            |
| M : 197000, N : 197000, K : 256, nnz : 2000000, sparsity : 99.9948%  | 2.10243 ms      |            |
| M : 390000, N : 390000, K : 256, nnz : 2000000, sparsity : 99.9987%  | 3.45181 ms      |            |
| M : 260000, N : 260000, K : 256, nnz : 4000000, sparsity : 99.9941%  | 4.01792 ms      |            |
| M : 241000, N : 241000, K : 256, nnz : 561000, sparsity : 99.999%    | 1.28944 ms      |            |
| M : 36000, N : 36000, K : 256, nnz : 4000000, sparsity : 99.6914%    | 1.33123 ms      |            |

太大的矩阵还是不能计算, 应该是时间太久了, 需要继续优化. 目前先针对 `matrixTileMultiplicationUseTensorCore2()`
函数中 `for` 循环的次数来优化.

```c++
for (int matrixPIdx = 0; matrixPIdx < nnz; ++matrixPIdx) {
    const size_t curRow = matrixSRowIndex[matrixPIdx];
    const size_t curCol = matrixSColIndex[matrixPIdx];

    int findLaneId, findIdx;
    positionCalculator(pRowId, pColId, curRow, curCol, findLaneId, findIdx);

    if (laneId == findLaneId) {
    matrixP[matrixPIdx] = cFrag.x[findIdx];
    }
}
```

---

### 使用COO格式储存稀疏矩阵进行运算(第二版)

针对 `for (int matrixPIdx = 0; matrixPIdx < nnz; ++matrixPIdx)` 循环进行了优化,
现在是 `for (int matrixPIdx = matrixTileIndex[warpId]; matrixPIdx < matrixTileIndex[warpId + 1]; ++matrixPIdx)`

#### 测试结果 行主序储存 16×16×16

- GPU : 4090
- Release build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 16 × 16 × 16

| M = 5000, N = 5000, k = 256 | sparsity | isratnisa_sddmm | zcx_sddmm | isratnisa_other | zcx_other | isratnisa | zcx     |
|-----------------------------|----------|-----------------|-----------|-----------------|-----------|-----------|---------|
| matrix_5000_5000_1250000    | 95%      | 1.09066         | 0.299328  | 14.4353         | 161.652   | 15.526    | 161.951 |
| matrix_5000_5000_1000000    | 96%      | 0.889664        | 0.29328   | 10.966          | 129.538   | 11.8574   | 129.831 |
| matrix_5000_5000_750000     | 97%      | 0.699264        | 0.283808  | 5.72211         | 97.0118   | 6.42138   | 97.2956 |
| matrix_5000_5000_500000     | 98%      | 0.490336        | 0.273024  | 3.36794         | 64.8893   | 3.85827   | 65.1624 |
| matrix_5000_5000_250000     | 99%      | 0.299968        | 0.260736  | 1.40698         | 32.4087   | 1.70694   | 32.6694 |
| matrix_5000_5000_125000     | 99.5%    | 0.263104        | 0.2048    | 1.02298         | 25.0018   | 1.28608   | 25.2066 |
| matrix_5000_5000_100000     | 99.6%    | 0.230624        | 0.179552  | 0.772224        | 21.3993   | 1.00285   | 21.5789 |
| matrix_5000_5000_75000      | 99.7%    | 0.21088         | 0.160448  | 0.585856        | 18.2028   | 0.796736  | 18.3632 |
| matrix_5000_5000_50000      | 99.8%    | 0.177344        | 0.140288  | 0.41792         | 14.9282   | 0.595264  | 15.0685 |
| matrix_5000_5000_25000      | 99.9%    | 0.155872        | 0.125984  | 0.238592        | 11.7914   | 0.394464  | 11.9173 |

| M = 10000, N = 10000, k = 256 | sparsity | isratnisa_sddmm | zcx_sddmm | isratnisa_other | zcx_other | isratnisa | zcx     |
|-------------------------------|----------|-----------------|-----------|-----------------|-----------|-----------|---------|
| matrix_10000_10000_5000000    | 95%      | 2.22109         | 1.02646   | 42.8288         | 1860.13   | 45.0571   | 1861.15 | 
| matrix_10000_10000_4000000    | 96%      | 1.75302         | 1.00064   | 35.4837         | 1485.79   | 37.2458   | 1486.79 |
| matrix_10000_10000_3000000    | 97%      | 1.33178         | 0.981952  | 26.5841         | 1114.52   | 27.9226   | 1115.5  |
| matrix_10000_10000_2000000    | 98%      | 0.922496        | 0.958176  | 18.8662         | 751.235   | 19.7887   | 752.193 |
| matrix_10000_10000_1000000    | 99%      | 0.504608        | 0.881696  | 5.77741         | 374.57    | 6.28202   | 375.452 | 

| M = 50000, N = 50000, k = 256 | sparsity | isratnisa_sddmm | zcx_sddmm | isratnisa_other | zcx_other | isratnisa | zcx    |
|-------------------------------|----------|-----------------|-----------|-----------------|-----------|-----------|--------|
| matrix_50000_50000_125000000  | 95%      | 47.681          | 25.3338   | 1642.71         |           | 1690.39   |        | 
| matrix_50000_50000_100000000  | 96%      | 38.7035         | 24.9057   | 1299.05         |           | 1337.75   |        | 
| matrix_50000_50000_75000000   | 97%      | 29.3539         | 24.4852   | 980.465         | 768576    | 1009.82   | 768600 | 
| matrix_50000_50000_50000000   | 98%      | 19.8487         | 24.0033   | 648.781         | 509691    | 668.629   | 509715 | 
| matrix_50000_50000_25000000   | 99%      | 10.4242         | 22.1125   | 323.567         | 255493    | 333.991   | 255515 |  

---

### 在上一版的基础上优化整理数据的函数(openTensorCoreModeForSampled())

#### 使用共享内存: pipelining方法.

一个线程计算一个warp中的.

##### 测试结果 行主序储存 16×16×16

- GPU : 4090
- Release build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 16 × 16 × 16

| M = 10000, N = 10000       | sparsity | k    | isratnisa_sddmm | zcx_sddmm | isratnisa_other | zcx_other | isratnisa | zcx     |
|----------------------------|----------|------|-----------------|-----------|-----------------|-----------|-----------|---------|
| matrix_10000_10000_5000000 | 95%      | 256  | 2.22109         | 1.02646   | 42.8288         | 1860.13   | 45.0571   | 1861.15 | 
| matrix_10000_10000_5000000 | 95%      | 500  | 4.94931         | 1.02646   | 49.456          | 1860.13   | 54.4053   | 1861.15 | 
| matrix_10000_10000_5000000 | 95%      | 1000 | 9.83683         | 0.981952  | 50.9666         | 1114.52   | 60.8035   | 1115.5  |
| matrix_10000_10000_5000000 | 95%      | 3000 | 29.3189         |           | 49.9806         |           | 79.2995   |         | 
| matrix_10000_10000_5000000 | 95%      | 5000 | 51.2411         |           | 50.1555         |           | 101.397   |         | 

| M = 50000, N = 50000, k = 256 | sparsity | isratnisa_sddmm | zcx_sddmm | isratnisa_other | zcx_other | isratnisa | zcx |
|-------------------------------|----------|-----------------|-----------|-----------------|-----------|-----------|-----|
| matrix_50000_50000_125000000  | 95%      |                 |           |                 |           |           |     | 
| matrix_50000_50000_100000000  | 96%      |                 |           |                 |           |           |     | 
| matrix_50000_50000_75000000   | 97%      |                 |           |                 |           |           |     | 
| matrix_50000_50000_50000000   | 98%      |                 |           |                 |           |           |     | 
| matrix_50000_50000_25000000   | 99%      |                 |           |                 |           |           |     | 

|                              | sparsity   | k   | isratnisa_sddmm | zcx_sddmm | isratnisa_other | zcx_other     | isratnisa   | zcx           |
|------------------------------|------------|-----|-----------------|-----------|-----------------|---------------|-------------|---------------|
| matrix_50000_50000_125000000 | 95.000008% | 256 | 47.755489       | 25.328480 | 2874.709961     | 675218.125000 | 2922.465332 | 675243.437500 |
| matrix_50000_50000_100000000 | 96.000000% | 256 | 39.461632       | 24.906912 | 2277.886719     | 540271.875000 | 2317.348389 | 540296.812500 |
| matrix_50000_50000_75000000  | 97.000000% | 256 | 29.235935       | 24.501247 | 1728.911499     | 405244.906250 | 1758.147461 | 405269.406250 |
| matrix_50000_50000_50000000  | 97.999992% | 256 | 19.853855       | 24.031233 | 1151.689331     | 270183.093750 | 1171.543213 | 270207.125000 |
| matrix_50000_50000_25000000  | 99.000000% | 256 | 10.436288       | 22.119232 | 574.328857      | 135090.375000 | 584.765137  | 135112.500000 |

|                            | sparsity   | k   | isratnisa_sddmm | zcx_sddmm | isratnisa_other | zcx_other   | isratnisa  | zcx         |
|----------------------------|------------|-----|-----------------|-----------|-----------------|-------------|------------|-------------|
| matrix_10000_10000_5000000 | 95.000000% | 256 | 2.287328        | 0.964608  | 102.048767      | 1273.916382 | 104.336098 | 1274.880981 |
| matrix_10000_10000_4000000 | 96.000000% | 256 | 1.836160        | 0.947200  | 80.397057       | 1022.330383 | 82.233215  | 1023.277588 |
| matrix_10000_10000_3000000 | 97.000000% | 256 | 1.406240        | 0.927648  | 60.102432       | 762.532471  | 61.508671  | 763.460144  |
| matrix_10000_10000_2000000 | 98.000000% | 256 | 0.992672        | 0.902976  | 43.708641       | 517.701538  | 44.701313  | 518.604492  |
| matrix_10000_10000_1000000 | 99.000000% | 256 | 0.582496        | 0.836480  | 18.417664       | 274.532379  | 19.000160  | 275.368866  |

|                         | sparsity   | k   | isratnisa_sddmm | zcx_sddmm | isratnisa_other | zcx_other | isratnisa | zcx       |
|-------------------------|------------|-----|-----------------|-----------|-----------------|-----------|-----------|-----------|
| matrix_5000_5000_125000 | 99.500000% | 256 | 0.254784        | 0.189440  | 2.306496        | 9.975552  | 2.561280  | 10.164991 |
| matrix_5000_5000_100000 | 99.599998% | 256 | 0.211328        | 0.190464  | 2.928640        | 9.227808  | 3.139968  | 9.418272  |
| matrix_5000_5000_75000  | 99.699997% | 256 | 0.207616        | 0.159648  | 1.467200        | 6.742848  | 1.674816  | 6.902496  |
| matrix_5000_5000_50000  | 99.800003% | 256 | 0.170624        | 0.154624  | 1.013760        | 5.224480  | 1.184384  | 5.379104  |
| matrix_5000_5000_25000  | 99.900002% | 256 | 0.159328        | 0.118656  | 0.582720        | 3.370688  | 0.742048  | 3.489344  |
| matrix_5000_5000_2500   | 99.989998% | 256 | 0.147072        | 0.039936  | 0.184320        | 1.727360  | 0.331392  | 1.767296  |

---

#### 使用共享内存: 分割处理

##### 测试结果 行主序储存 16×16×16

- GPU : 4090
- Release build
- matrixA(half) : row_major, matrixB(half) : row_major, matrixP(float) : row_major
- WMMA : 16 × 16 × 16

---