![封面](img/[论文笔记]探索注意力稀疏性以加速GPU上的Transformer训练/封面.png)

# [论文笔记]探索注意力稀疏性以加速GPU上的Transformer训练

Exploring Attention Sparsity to Accelerate Transformer Training on GPUs

---

## 简介

Transformer是一种用于解决各种序列任务(包括序列分类和序列到序列任务)的先进神经网络.
为了捕捉序列中数据节点之间的关系, Transformer在训练过程中需要大量的多头注意力(MHA)操作.
特别是,MHA操作的计算复杂度随着输入序列长度的增长而呈二次方增长.
因此, 先前的几种方法视图通过稀疏化注意力得分矩阵来降低计算复杂度并提高MHA计算的数据局部性.

---

## 背景与相关工作

### 变压器

仅编码器的Transformer是Transformer模型的一种变体, 被用于涉及多种分类任务的场景, 包括但不限于文本和图像数据集.
算法1展示了原始仅编码器的Transformer的钱箱传播伪代码, 其中每个编码器层由一个多投注意力(MHA)子层和一个前瞻子层组成.

### 相关工作

---

## 动机

为了识别多头自注意力(MHA)操作中的稀疏模式, 论文使用了用于图像分类的预训练视觉变换器(ViT)和文本处理的预训练BERT模型进行了文本分类.
ViT是在ImageNet-21K图形数据集上进行预训练的, 而BERT则是在维基百科和BookCorpus文本数据集上进行预训练的.
图1展示了在微型微调的第一个时期中, 随机选取的图像和文本分类任务的注意力得分矩阵$A^s$.
在微调过程中, 使用CIFAR-10退选哪个数据集进行图像分类, 使用MRPC文本数据集进行文本分类任务.

结果清晰表明, 矩阵$A^s$中的大多数元素都接近于0, 并且在不同数据集合任务中, $A^s$的稀疏模式各不相同.
这表明输入序列中只有少数数据点之间存在关联, 并且这种关联可能因数据集和任务的不同而有所变化.

如图1左侧所示, 图像分类任务的注意力得分矩阵在对角线上呈现较大的值, 这表明数据点于其自身更具有相关性.
此外, 在对角线附近的对角线上也显示出较大的值, 这表明相邻的数据点之间存在重要的关系.
而且, 由于图像数据的二维特性(不含RGB通道), 矩阵中对角线的模式呈现出一致的间隔, 其中垂直相邻的数据点可能具有显著的关系.

图1右侧所示, 从文本分类任务中获得的注意力得分矩阵在右侧大多为0值. 会议这是因为文本数据通常长度不宜, 可能需要相对较短的输入序列进行填充.
因此, 这些这些零填充数据点的注意力得分矩阵的值为0. 此外, 由于主对角线上的值较大, 表明自身之间的关系很重要, 这与图像分类任务相似.
另外, 具有较大的值的较粗对角线可以解释为更多的相邻数据点之间存在显著的关系.

基于实验, 认识到对于特定领域的数据集和任务, 动态捕捉注意力得分矩阵的稀疏模式是必要的的.

---

## SAT: 稀疏感知性变压器

## SAT概述

在本小节中, 将对新SAT模型进行高层次概述, 该模型在训练Transformer期间能够动态识别注意力得分矩阵的稀疏模式,
并对标准的多头注意力(MHA)操作进行稀疏化处理. SAT模型能够在从头开始训练模型或使用预训练的Transformer进行微调的过程中对MHA操作进行稀疏块化处理.
如图2a所示, 将SAT的整体训练过程分为三个阶段: 稠密MHA训练遵循与原始Transformer相同的训练流程, 对MHA中的所有元素进行操作.
在稠密MHA训练阶段, 与标准MHA类似, 会执行两次稠密矩阵-稠密矩阵乘法(GEMM)操作以及一次标准的softmax计算(图2b),
直到每个编码器层的注意力得分矩阵$A^s$展现出特定的稀疏模式.



---

## 实验评估

设备:

- CPU AMD Ryzen Threadripper PRO 5995WX(16核, 32线程, 128GB RAM)
- GPU 4台 NVIDIA RTX A5000(24GB Global Memory, 64 SMs, 6 MB L2 cache)

数据集:

- 图像分类: 使用了 CIFAR-10 和 iNaturalist 2018 数据集进行图像分类. 前者包含32×32像素的图像, 而后者则被缩放至这一分辨率.
  每张图像都被视为1024个像素组成的序列, 每个像素对应输入序列中的一个数据点. 模型使用CIFAR-10数据集将图像分类为10个互斥的类别,
  使用INaturalist数据集将其分类为14个超类别(植物, 昆虫, 鸟类, 哺乳类等).
- 文本分类: 使用了AG News 和Yelp Review数据集, 每个数据集的最大序列长度为1024, 其中输入序列中的每个字符都被视为一个数据点.
  模型使用AG News数据集将文本分类为4个主题, 并对Yelp评价进行情感分类, 将1星和2星评价视为负面评价, 将3星和4星评价视为正面评价.

对比模型:

将原始的仅编码器Transformer以及三种最先进的稀疏Transformer(Longformer, BigBird和LSG Attention)的性能与SAT模型进行比较.

- 原始的仅编码器Transformer: 此实验基于原始的Transformer架构, 并在训练过程中执行原始的MHA操作.
- Longformer: 该模型采用基于扩张滑动窗口的稀疏注意力机制. 评估时采用的滑动窗口大小为64.
- BigBird: 该模型融合了稀疏注意力机制, 包括滑动窗口注意力, 全局注意力和随机注意力. 评估采用32的块大小和3个随机块.
- LSG Attention: 该模型融合了局部注意力,稀疏注意力和全局注意力. 稀疏注意力采用最大范数进行评估. 评估时采用32的块大小和32的稀疏块大小,
  稀疏因子为2.
- SAT: 利用可学习的参数来捕获训练数据集的一般稀疏模式.

实验中, 嵌入维度(D)设为64, 批量大小由可用内存大小决定, 最终确定为512. 可学习参数的块大小(B)设为32. 对于AG News,
CIFAR-10和iNaturalist数据集, 偏度阈值($\theta$)设为1.7, 对于Yelp Review数据集则设为1.3. 所有数据集的差异阈值($\delta$)
固定为1.3. 所有实验结果均为五次实验的平均值.

下表展示了五个模型在两个分类任务中四个不同数据集上的准确率. 在文本分类任务中, SAT模型的准确率高于其他所有基准模型.
然而在图像分类任务中, 准确率略低于其他最先进的模型. 原始Transformer模型在所有数据集上的准确率都是最低的,
这表明在多头注意力机制(MHA)中对注意力得分矩阵的所有元素进行操作可能会给模型的质量带来噪音. 换句话来说, 某些元素比其他元素更重要,
运行不重要的元素存在会对模型产生负面影响.

![两项分类任务准确率的比较](img/[论文笔记]探索注意力稀疏性以加速GPU上的Transformer训练/两项分类任务准确率的比较.png)

下图展示了四个数据集上每一步的平均训练时间. 在文本分类任务中, SAT模型在AG News数据集上的训练速度比BigBird快了2.48倍,
在Yelp Review数据集上快了2.45倍. 在实现更好的文本分类准确率的同时, SAT显著减少了整体训练时间.
这一结果表明, SAT通过CUDA内核有效地捕捉了注意力得分矩阵中的重要元素, 并优化了稀疏多头注意力(MHA)操作. 同时, 在图像分类任务中,
在CIFAR-10和iNaturalist数据集上分别比BigBird快了2.84倍和2.74倍. 尽管在图像数据集上的准确率略有降低, 但SAT显著减少了训练时间.

![分类任务中每一步的训练时间对比](img/[论文笔记]探索注意力稀疏性以加速GPU上的Transformer训练/分类任务中每一步的训练时间对比.png)

下图展示了在原始Transformer中运行密集型MHA操作以及在SAT中执行MHA操作时, 在四个不同数据集上所用时间的分解情况.
对于CIFAR-10数据集, 将Q和K的GEMM操作替换为SDDMM操作, 速度提升了1.55倍. softmax操作实现了显著的37.34倍的速度提升.
替换$A^s$和V的GEMM操作的SpMM操作, 速度提升了2.34倍. 这一结果表明, 原始Transformer模型中实现的softmax函数是主要瓶颈.

![四个数据集上原始密集型多头注意力(MHA)操作和所提出的稀疏型MHA操作的时间分解情况](img/[论文笔记]探索注意力稀疏性以加速GPU上的Transformer训练/四个数据集上原始密集型多头注意力(MHA)操作和所提出的稀疏型MHA操作的时间分解情况.png)

SAT模型执行通过考虑稀疏性来优化softmax函数, softmax操作的执行时间显著缩短. SAT在所有任务中, 与多头注意力(MHA)
机制相关的每个操作中都实现了加速.

---

## 结论

随着数据序列长度的增加, Transformer模型中多头注意力(MHA)操作的计算复杂度呈二次增长. 因此, 论文开发了一种新颖的稀疏化方法,
该方法在大幅减少MHA计算所需操作数量的同时, 还能保留序列中数据点之间的重要关系.

---